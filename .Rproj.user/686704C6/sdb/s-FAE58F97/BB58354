{
    "collab_server" : "",
    "contents" : "library(data.table)\nlibrary(glmnet)\nlibrary(gbm)\nlibrary(xgboost)\nlibrary(tidyverse)\nlibrary(doParallel)\ndb=rbind(cbind(fread(\"train.csv\"),train=1),cbind(fread(\"test.csv\"),SalePrice=1,train=0))\nvar_useless=c(\"Id\",\"train\")\n\n# char=>factor\ndb <- db %>%\n  mutate_if(sapply(db, is.character), as.factor)\n\n# NA as a level\ndb <- db %>% \n  mutate_if(sapply(db, is.factor), addNA)\n\n# scale numeric\nscale_vec <- function(x){\n  c(scale(x))\n}\nnums <- names(sapply(db,is.numeric))\ndb <- db%>%\n  mutate_if(setdiff(nums,c(\"train\",\"SalePrice\")),scale_vec)\n\nsum(sapply(db,function(x)sd(x)==1)) #no numerical constant\nmin(sapply(db,function(x)length(levels(factor(x))))) # no factor constant\n\n#in this dbset, NA actually means \"there isn't\", not \"we don't know\"...\nNAs <- sapply(db,function(x)sum(is.na(x))/nrow(db))\nNAs <- NAs[NAs>0]\nsapply(db[,names(NAs)],summary)\n\n#LotFrontage NA is none => 0\ndb[is.na(db$LotFrontage),]$LotFrontage <- 0\n#MasVnrArea NA is none => 0\ndb[is.na(db$MasVnrArea),]$MasVnrArea <- 0\ndb[is.na(db$BsmtHalfBath),]\nfor (nm in setdiff(names(NAs),names(db)[grep(pattern=\"Yr\",x = names(db),ignore.case = T)])){\n  print(nm)\n  db[is.na(db[[nm]]),nm]<-0\n}\nlibrary(Hmisc)\ndb$GarageYrBlt_cut=addNA(cut2(db$GarageYrBlt,g = 10))\ndb=within(db,rm(GarageYrBlt))\n\n##########################\n#### ONE HOT ENCODING ####\n##########################\n# var_factor=names(db)[sapply(db,is.factor)]\n# db <- with(db,\n#      data.frame(model.matrix(~.-1,db[,var_factor]),\n#                 db[,setdiff(names(db),var_factor)]))\n\n# decreases the performance ! takes A LOT MORE time and give significantly less good results.\n# depth.depth shrinkage.shrinkage        bag.fraction      n.minobsinnode             n.trees \n# 5.0000000           0.0050000           0.3000000          10.0000000        2000.0000000 \n# nb_split.nb_split                RMSE \n# 10.0000000           0.1279916 \n# and even with very slow learning\n# Using 10000 trees...\n# depth.depth shrinkage.shrinkage        bag.fraction      n.minobsinnode             n.trees   nb_split.nb_split                RMSE \n# 8.000000e+00        1.000000e-03        3.000000e-01        1.000000e+01        1.000000e+04        1.000000e+01        1.276744e-01 \n\n\n#GarageYrBlt don't know when the garage was build ? but is there a garage ?\n#sample_train=sample(1:sum(db$train),round(.5*sum(db$train)))\ntest_db=db%>%filter(train==0)\ndb_matrix=model.matrix(~.,data = db)\n\ntrain_db=db%>%filter(train==1)\ntrain_id <- which(colnames(db_matrix)==\"train\")\n\ntest_matrix=db_matrix[db_matrix[,train_id]==0,]\ntrain_db=train_db[,setdiff(names(db),var_useless)]\ntrain_db$logSalePrice=log(train_db$SalePrice)\nrandom=sample(1:nrow(train_db))\n\niter=c(depth=0,shrinkage=0,bag.fraction=0,n.minobsinnode=0,n.trees=0,nb_split=0,RMSE_glm=0,RMSE_gbm=0)\n\nnb_split=10\n# best currently is without one hot encoding\n#depth.depth shrinkage.shrinkage        bag.fraction      n.minobsinnode             n.trees \n#5.0000000           0.0050000           0.3000000          10.0000000        2000.0000000 \n#nb_split.nb_split                RMSE \n#10.0000000           0.1257294 \n\n\n# Using 5886 trees...\n# depth.depth shrinkage.shrinkage        bag.fraction      n.minobsinnode             n.trees   nb_split.nb_split                RMSE \n# 8.000000e+00        1.000000e-03        3.000000e-01        1.000000e+01        1.000000e+04        1.000000e+01        1.255727e-01 \ncl <- makeCluster(spec=6)\nregisterDoParallel(cl = cl)\n\n\n\ngrid=expand.grid(depth=c(10),nb_split=c(10),shrinkage=c(.0001),bag.fraction=c(.1,.3,.5),n.minobsinnode=c(40,15,5))\n#x=grid[1,]\n\nresults <- foreach(i=1:nrow(grid),.combine=cbind,.packages = c(\"gbm\",\"glmnet\",\"tidyverse\",\"data.table\")) %dopar%{\n  x=c(grid[i,])\n  predictions_table <- data.frame(\"id\"=1:nrow(test_db))\n  depth=x[[1]]\n  nb_split=x[[2]]\n  shrinkage=x[[3]]\n  n.trees=10/shrinkage\n  bag.fraction=x[[4]]\n  n.minobsinnode=x[[5]]\n\n  \n  step_size=round(nrow(train_db)/nb_split)\n  size=c()\n  SS_gbm=c()\n  SS_glm=c()\n  \n  for (i in 1:nb_split){\n    if(i==nb_split){\n      sample_train=seq(from = (i-1)*step_size+1,to = nrow(train_db),by = 1)\n    }\n    else sample_train=seq((i-1)*step_size+1,i*step_size)\n    # if (n.minobsinnode>=bag.fraction*(step_size)){\n    #   print(paste(\"minobsinnode\",n.minobsinnode))\n    #   print(paste(\"bag fraction x sample_size\",bag.fraction*(nrow(train_db)-length(sample_train))))\n    #   print(paste(\"bag fraction x sample_size\",bag.fraction*(length(sample_train))))\n    #   print(\"gonna crash\")\n    # }\n    ##########################\n    ####      TRY GBM     ####\n    ##########################\n  var_to_keep=setdiff(names(train_db),\"SalePrice\")\n  gbm_1 <- gbm(logSalePrice~.,data=train_db[-sample_train,var_to_keep],n.minobsinnode = n.minobsinnode,distribution=\"gaussian\",\n                   interaction.depth=depth,n.trees = n.trees,train.fraction = .7,shrinkage = shrinkage,bag.fraction = bag.fraction,verbose = F)\n  var_imp <- summary(gbm_1)\n  selected_var <- gsub(pattern = \"`\",replacement = \"\",c(as.character(var_imp[var_imp$rel.inf>.1,]$var)))\n  ##########################\n    ####    TRY GLMNET    ####\n    ##########################\n  offset=predict(gbm_1,train_db,n.trees = which.min(gbm_1$valid.error))\n  var_to_keep_postGBM=colnames(db_matrix)[(unlist(sapply(selected_var,FUN = function(x)grep(pattern = x,x = colnames(db_matrix)))))]\n  train_matrix=db_matrix[db_matrix[,train_id]==1,var_to_keep_postGBM][-sample_train,] #CAREFUL rows with NAs are removed https://stackoverflow.com/questions/6447708/model-matrix-generates-fewer-rows-than-original-data-frame\n  train_matrix\n  dim(train_matrix)\n  target=train_db[-sample_train,]$logSalePrice\n  length(target)\n  glm_1 <- glmnet(x=train_matrix,y = target,family=\"gaussian\",alpha=0.5,standardize=F,nlambda = 100,offset=offset[-sample_train])\n\n  valid_matrix=db_matrix[db_matrix[,train_id]==1,var_to_keep_postGBM][sample_train,]  #CAREFUL rows with NAs are removed https://stackoverflow.com/questions/6447708/model-matrix-generates-fewer-rows-than-original-data-frame\n  test_matrix=db_matrix[db_matrix[,train_id]==0,var_to_keep_postGBM]\n  ##########################\n    ####    TRY XGBOOST   ####\n    ##########################\n    \n      #   param <- list(max_depth = 6, eta = .01,subsample=.4,colsample_bytree=.5,lambda=.1,lambda_bias=.1,alpha=.1)\n  #   data_samp <- data.matrix(train_db[-sample_train,setdiff(names(train_db),c(\"SalePrice\",\"logSalePrice\"))])\n  #   label_samp <- train_db[-sample_train,\"logSalePrice\"]\n  #   valid_samp <- sample(1:nrow(data_samp),round(.3*nrow(data_samp)))\n  #   dtrain <- xgb.DMatrix(data_samp[-valid_samp,], label=label_samp[-valid_samp])\n  #   watchlist <- list(eval=xgb.DMatrix(data_samp[valid_samp,],label=label_samp[valid_samp]),train=dtrain)\n  # gbm_1 <- xgb.train(data = dtrain,\n  #                  verbose = 1,early_stopping_rounds = 50,params = param,nrounds = 10000,watchlist)\n  pred_1 <- predict(gbm_1,train_db[sample_train,setdiff(names(train_db),\"SalePrice\")],n.trees = which.min(gbm_1$valid.error))\n  pred_2 <- predict(glm_1,valid_matrix,newoffset = offset[sample_train])\n  obs_1 <- train_db[sample_train,\"logSalePrice\"]\n  \n  nm_gbm=paste0(\"pred_gbm_\",i)\n  gbm_predict_test=predict(gbm_1,test_db,n.trees = which.min(gbm_1$valid.error))\n  predictions_table=cbind(predictions_table,x=gbm_predict_test)\n  setnames(predictions_table,\"x\",nm_gbm)\n  \n  nm_glm=paste0(\"pred_glm_\",i)\n  lambda_predictions=data.frame(x=predict(glm_1,newx = valid_matrix,newoffset = offset[sample_train]))\n  compute_RMSE <- function(x){\n    sqrt(sum((x-obs_1)^2))\n  }\n  sapply(lambda_predictions,compute_RMSE)\n  \n  predictions_table=cbind(predictions_table,data.frame(x=predict(glm_1,newx = test_matrix,s = tail(glm_1$lambda,1),newoffset = gbm_predict_test)))\n  setnames(predictions_table,\"X1\",nm_glm)\n  \n  \n  SS_gbm=c(SS_gbm,(pred_1-obs_1)^2)\n  SS_glm=c(SS_glm,(pred_2-obs_1)^2)\n  \n  size=c(size,length(sample_train))\n}\nRMSE_glm=sqrt(sum(SS_glm)/sum(size))\nRMSE_gbm=sqrt(sum(SS_gbm)/sum(size))\n\niter=c(depth=depth,shrinkage=shrinkage,bag.fraction=bag.fraction,n.minobsinnode=n.minobsinnode,n.trees=n.trees,nb_split=nb_split,RMSE_glm=RMSE_glm,RMSE_gbm=RMSE_gbm)\nprint(iter)\n\nreturn(predictions_table)\n\n}\n\n# results <- do.call(cbind,results)\nsave(list = \"results\",file = \"results.RData\")\npredictors <- grep(pattern = \"pred\",x = names(results))\nprediction <- rowSums(results[,predictors])/length(predictors)\n\nplot(prediction,ylim=c(11,13))\n\ntest_db$predict=exp(prediction)\n\nplot(test_db$predict,ylim=c(10,12))\n\nto_submit=test_db[,c(\"Id\",\"predict\")]\nsetnames(to_submit,\"predict\",\"SalePrice\")\nwrite_csv(to_submit,path = \"submit_GBM10000GLM_offset.csv\")\n\nstopCluster(cl)\n",
    "created" : 1506603439109.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3282310743",
    "id" : "BB58354",
    "lastKnownWriteTime" : 1507278841,
    "last_content_update" : 1507278841875,
    "path" : "~/Documents/HousePrices/gbm_ensembling.R",
    "project_path" : "gbm_ensembling.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}